{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ddd5bf-7615-45a2-bd9c-9827e7c2b813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/home/dwh48@drexel.edu/sparse_coding_torch')\n",
    "\n",
    "from feature_extraction.conv_sparse_model import ConvSparseLayer\n",
    "from data_classifiers.small_data_classifier import SmallDataClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from feature_extraction.train_conv3d_sparse_model import load_balls_data\n",
    "from feature_extraction.train_conv3d_sparse_model import plot_original_vs_recon\n",
    "from feature_extraction.train_conv3d_sparse_model import plot_filters\n",
    "from feature_extraction.train_conv3d_sparse_model import plot_video\n",
    "\n",
    "from utils.load_data import load_bamc_clips, load_covid_data\n",
    "\n",
    "from IPython.display import HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86142a5-930b-4a0c-ab7a-6be63a1d1f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == \"cpu\":\n",
    "    batch_size = 1\n",
    "else:\n",
    "    batch_size = 4*3\n",
    "    # batch_size = 3\n",
    "\n",
    "# train_loader = load_balls_data(batch_size)\n",
    "train_loader, test_loader = load_bamc_clips(batch_size, 0.8, sparse_model=None, device=None, num_frames=4, seed=42)\n",
    "print('Loaded', len(train_loader), 'train examples')\n",
    "print('Loaded', len(test_loader), 'test examples')\n",
    "\n",
    "example_data = next(iter(train_loader))\n",
    "\n",
    "sparse_layer = ConvSparseLayer(in_channels=1,\n",
    "                               out_channels=64,\n",
    "                               kernel_size=(4, 16, 16),\n",
    "                               stride=1,\n",
    "                               padding=0,\n",
    "                               convo_dim=3,\n",
    "                               rectifier=True,\n",
    "                               lam=0.05,\n",
    "                               max_activation_iter=75,\n",
    "                               activation_lr=1e-2)\n",
    "model = sparse_layer\n",
    "model = torch.nn.DataParallel(model, device_ids=[0, 1, 2, 3])\n",
    "model.to(device)\n",
    "\n",
    "learning_rate = 3e-4\n",
    "optimizer = torch.optim.Adam(sparse_layer.parameters(),\n",
    "                                    lr=learning_rate)\n",
    "#optimizer = torch.optim.SGD(sparse_layer.parameters(),\n",
    "#                            momentum=0.9,\n",
    "#                            lr=learning_rate)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "# scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-14, max_lr=1e-5,step_size_up=20,mode=\"triangular2\")\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d00195-42c4-4e29-9a9e-ada194cc6f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models if we'd like to\n",
    "checkpoint = torch.load(\"/home/dwh48@drexel.edu/sparse_coding_torch/model-20211027-034737.pt\")\n",
    "model.module.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# model.module.import_opencv_dir('/home/dwh48@drexel.edu/sparse_coding_torch/eds_weights')\n",
    "\n",
    "# Put everything on the target device\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca61d5d-4f75-4409-b860-5d453b17e856",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d607af8e-7dd1-4bb9-9578-95c279465f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for g in optimizer.param_groups:\n",
    "#     g['lr'] = 1e-5\n",
    "# sparse_layer.lam = 0.1\n",
    "# sparse_layer.shrink = 0.1\n",
    "\n",
    "loss_log = []\n",
    "best_so_far = float('inf')\n",
    "\n",
    "for epoch in range(300):\n",
    "    epoch_loss = 0\n",
    "    epoch_start = time.perf_counter()\n",
    "    # for local_batch in train_loader:\n",
    "    for labels, local_batch in train_loader:\n",
    "        local_batch = local_batch.to(device)\n",
    "        \n",
    "        # pred, activations = model(local_batch)\n",
    "        activations = model(local_batch)\n",
    "        loss = sparse_layer.loss(local_batch, activations)\n",
    "        # loss += criterion(pred, torch_labels)\n",
    "        # print('epoch={}, loss={:.2f}, time={:.2f}'.format(epoch, loss, t2-t1))\n",
    "        epoch_loss += loss.item() * local_batch.size(0)\n",
    "        # print('l:', loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        sparse_layer.normalize_weights()\n",
    "    \n",
    "    epoch_end = time.perf_counter()    \n",
    "    epoch_loss /= len(train_loader.sampler)\n",
    "    \n",
    "    if epoch_loss < best_so_far:\n",
    "        print(\"found better model\")\n",
    "        # Save model parameters\n",
    "        torch.save({\n",
    "            'model_state_dict': model.module.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, datetime.now().strftime(\"saved_models/sparse_conv3d_model-best.pt\"))\n",
    "        best_so_far = epoch_loss\n",
    "        \n",
    "    loss_log.append(epoch_loss)\n",
    "    print('epoch={}, epoch_loss={:.2f}, time={:.2f}'.format(epoch, epoch_loss, epoch_end - epoch_start))\n",
    "    # scheduler.step(epoch_loss)\n",
    "    # scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d47461-8c5d-446f-bfe6-1b823a75e5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24decdf-d7dc-4a37-8877-e85742ece6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6ff08a-ae7d-4bff-8106-a624427d8f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model parameters\n",
    "torch.save({\n",
    "    'model_state_dict': model.module.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, datetime.now().strftime(\"saved_models/sparse_conv3d_model-%Y%m%d-%H%M%S.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef38348-6bcb-4dda-8583-4e86fdf8099d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ani = plot_video(example_data[1][1])\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37229a1d-4762-4f3f-a7c1-9b5f98ca637b",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=1\n",
    "u_init = torch.zeros([1, sparse_layer.out_channels] +\n",
    "                    sparse_layer.get_output_shape(example_data[1]))\n",
    "\n",
    "activations, _ = sparse_layer(example_data[1][idx:idx+1].to(device), u_init)\n",
    "print(torch.count_nonzero(activations) / torch.numel(activations))\n",
    "reconstructions = sparse_layer.reconstructions(\n",
    "    activations).cpu().detach().numpy()\n",
    "\n",
    "ani = plot_video(reconstructions.squeeze(0))\n",
    "# ani = plot_original_vs_recon(example_data[1][idx:idx+1], reconstructions, idx=0)\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b2ee57-4ae4-4a55-a0cb-6b265487050e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ani = plot_filters(sparse_layer.filters.cpu().detach())\n",
    "HTML(ani.to_html5_video())\n",
    "# ani.save(\"/home/dwh48@drexel.edu/sparse_coding_torch/eds_vis.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e2ae8a-cf00-4744-b57c-6fd0fa22735d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SmallDataClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, sparse_layer):\n",
    "        super().__init__()\n",
    "\n",
    "        self.sparse_layer = sparse_layer\n",
    "        \n",
    "        self.pool = nn.MaxPool3d(2, 2)\n",
    "        \n",
    "        self.dropout3d = torch.nn.Dropout3d(p=0.1, inplace=False)\n",
    "        self.dropout = torch.nn.Dropout(p=0.5, inplace=False)\n",
    "        \n",
    "        # First fully connected layer\n",
    "        self.fc1 = nn.Linear(3272808, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    # x represents our data\n",
    "    def forward(self, x):\n",
    "        # Pass data through conv1\n",
    "        activations = self.sparse_layer(x)\n",
    "        \n",
    "        # x = self.dropout3d(x)\n",
    "        \n",
    "        # Flatten x with start_dim=1\n",
    "        x = torch.flatten(activations, 1)\n",
    "        \n",
    "        # print(x.shape)\n",
    "        \n",
    "        # Pass data through fc1\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490cf8c8-db9e-4c14-b9f0-418e1893ea05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a frozen sparse layer then add a small data classifier on top\n",
    "frozen_sparse = ConvSparseLayer(in_channels=1,\n",
    "                               out_channels=24,\n",
    "                               kernel_size=16,\n",
    "                               stride=2,\n",
    "                               padding=0,\n",
    "                               convo_dim=3,\n",
    "                               rectifier=True,\n",
    "                               shrink=0.05,\n",
    "                               lam=0.05,\n",
    "                               max_activation_iter=300,\n",
    "                               activation_lr=1e-2)\n",
    "sparse_param = torch.load(\"saved_models/sparse_conv3d_model-best.pt\")\n",
    "frozen_sparse.load_state_dict(sparse_param['model_state_dict'])\n",
    "        \n",
    "for param in frozen_sparse.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "predictive_model = torch.nn.DataParallel(SmallDataClassifier(frozen_sparse), device_ids=[0,1,2,3])\n",
    "predictive_model.to(device)\n",
    "\n",
    "learning_rate = 1e-2\n",
    "prediction_optimizer = torch.optim.Adam(predictive_model.parameters(),\n",
    "                                        lr=learning_rate)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689cb611-36c6-4c39-bd19-fab35d953df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "predictive_model.to(device)\n",
    "\n",
    "idx=3\n",
    "predictive_model(example_data[1][idx:idx+1].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b91fc8-3dd6-4f55-9008-5b5a6950c46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for epoch in range(15):\n",
    "    epoch_loss = 0\n",
    "    # for local_batch in train_loader:\n",
    "    t1 = time.perf_counter()\n",
    "    for labels, local_batch in train_loader:\n",
    "        local_batch = local_batch.to(device)\n",
    "        \n",
    "        torch_labels = torch.zeros(len(labels))\n",
    "        torch_labels[[i for i in range(len(labels)) if labels[i] == 'PTX_No_Sliding']] = 1\n",
    "        torch_labels = torch_labels.unsqueeze(1).to(device)\n",
    "        \n",
    "        pred, activations = predictive_model(local_batch)\n",
    "        \n",
    "        loss = criterion(pred, torch_labels)\n",
    "        # loss += frozen_sparse.loss(local_batch, activations)\n",
    "        epoch_loss += loss.item() * local_batch.size(0)\n",
    "        \n",
    "        prediction_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        prediction_optimizer.step()\n",
    "        \n",
    "    t2 = time.perf_counter()\n",
    "    print('epoch={}, time={:.2f}, loss={:.2f}'.format(epoch, t2-t1, epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60afb31-7c80-48e7-a920-80f4cab1e0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    y_h = None\n",
    "    y = None\n",
    "    \n",
    "    error = None\n",
    "    \n",
    "    t1 = time.perf_counter()\n",
    "    # for local_batch in train_loader:\n",
    "    for labels, local_batch in test_loader:\n",
    "        local_batch = local_batch.to(device)\n",
    "\n",
    "        torch_labels = torch.zeros(len(labels))\n",
    "        torch_labels[[i for i in range(len(labels)) if labels[i] == 'PTX_No_Sliding']] = 1\n",
    "        torch_labels = torch_labels.unsqueeze(1).to(device)\n",
    "\n",
    "        \n",
    "        pred, _ = predictive_model(local_batch)\n",
    "        \n",
    "        loss = criterion(pred, torch_labels)\n",
    "        epoch_loss += loss.item() * local_batch.size(0)\n",
    "\n",
    "        if error is None:\n",
    "            error = torch.abs(torch_labels - torch.nn.Sigmoid()(pred).round()).flatten()\n",
    "            y_h = torch.nn.Sigmoid()(pred).round().flatten()\n",
    "            y = torch_labels.flatten()\n",
    "        else:\n",
    "            error = torch.cat((error, torch.abs(torch_labels - torch.nn.Sigmoid()(pred).round()).flatten()))\n",
    "            y_h = torch.cat((y_h, torch.nn.Sigmoid()(pred).round().flatten()))\n",
    "            y = torch.cat((y, torch_labels.flatten()))\n",
    "            \n",
    "    t2 = time.perf_counter()\n",
    "    \n",
    "    print('loss={:.2f}, time={:.2f}'.format(loss, t2-t1))\n",
    "        \n",
    "    print(\"Overall error={:.2f}\".format(error.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2caa69-f3dc-4847-8166-68d32220cbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y.cpu(), y_h.cpu())\n",
    "\n",
    "cm_display = ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b932c9-2c72-449d-a9f2-a246e3ddd325",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9342754c-5175-40ec-8412-48b457dd360e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c95423e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from conv_sparse_model import ConvSparseLayer\n",
    "from small_data_classifier import SmallDataClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from train_conv3d_sparse_model import load_balls_data\n",
    "from train_conv3d_sparse_model import plot_original_vs_recon\n",
    "from train_conv3d_sparse_model import plot_filters\n",
    "from train_conv3d_sparse_model import plot_video\n",
    "\n",
    "from load_data import load_bamc_data, load_covid_data\n",
    "\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb331ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == \"cpu\":\n",
    "    batch_size = 1\n",
    "else:\n",
    "    batch_size = 4*3\n",
    "    # batch_size = 3\n",
    "\n",
    "# train_loader = load_balls_data(batch_size)\n",
    "train_loader, test_loader = load_bamc_data(batch_size)\n",
    "print('Loaded', len(train_loader), 'train examples')\n",
    "print('Loaded', len(test_loader), 'test examples')\n",
    "\n",
    "example_data = next(iter(train_loader))\n",
    "\n",
    "sparse_layer = ConvSparseLayer(in_channels=1,\n",
    "                               out_channels=24,\n",
    "                               kernel_size=16,\n",
    "                               stride=2,\n",
    "                               padding=0,\n",
    "                               convo_dim=3,\n",
    "                               rectifier=True,\n",
    "                               shrink=0.05,\n",
    "                               lam=0.05,\n",
    "                               max_activation_iter=300,\n",
    "                               activation_lr=1e-2)\n",
    "model = sparse_layer\n",
    "model = torch.nn.DataParallel(model, device_ids=[0, 1, 2, 3])\n",
    "model.to(device)\n",
    "\n",
    "learning_rate = 3e-4\n",
    "optimizer = torch.optim.Adam(sparse_layer.parameters(),\n",
    "                                    lr=learning_rate)\n",
    "#optimizer = torch.optim.SGD(sparse_layer.parameters(),\n",
    "#                            momentum=0.9,\n",
    "#                            lr=learning_rate)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "# scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-14, max_lr=1e-5,step_size_up=20,mode=\"triangular2\")\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb77833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models if we'd like to\n",
    "checkpoint = torch.load(\"saved_models/sparse_conv3d_model-best.pt\")\n",
    "model.module.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# Put everything on the target device\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0240a27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f83676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for g in optimizer.param_groups:\n",
    "#     g['lr'] = 1e-5\n",
    "# sparse_layer.lam = 0.1\n",
    "# sparse_layer.shrink = 0.1\n",
    "\n",
    "loss_log = []\n",
    "best_so_far = float('inf')\n",
    "\n",
    "for epoch in range(300):\n",
    "    epoch_loss = 0\n",
    "    epoch_start = time.perf_counter()\n",
    "    # for local_batch in train_loader:\n",
    "    for labels, local_batch in train_loader:\n",
    "        local_batch = local_batch.to(device)\n",
    "        \n",
    "        # pred, activations = model(local_batch)\n",
    "        activations = model(local_batch)\n",
    "        loss = sparse_layer.loss(local_batch, activations)\n",
    "        # loss += criterion(pred, torch_labels)\n",
    "        # print('epoch={}, loss={:.2f}, time={:.2f}'.format(epoch, loss, t2-t1))\n",
    "        epoch_loss += loss.item() * local_batch.size(0)\n",
    "        # print('l:', loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        sparse_layer.normalize_weights()\n",
    "    \n",
    "    epoch_end = time.perf_counter()    \n",
    "    epoch_loss /= len(train_loader.sampler)\n",
    "    \n",
    "    if epoch_loss < best_so_far:\n",
    "        print(\"found better model\")\n",
    "        # Save model parameters\n",
    "        torch.save({\n",
    "            'model_state_dict': model.module.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, datetime.now().strftime(\"saved_models/sparse_conv3d_model-best.pt\"))\n",
    "        best_so_far = epoch_loss\n",
    "        \n",
    "    loss_log.append(epoch_loss)\n",
    "    print('epoch={}, epoch_loss={:.2f}, time={:.2f}'.format(epoch, epoch_loss, epoch_end - epoch_start))\n",
    "    # scheduler.step(epoch_loss)\n",
    "    # scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fa6dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e531a6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912db11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model parameters\n",
    "torch.save({\n",
    "    'model_state_dict': model.module.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, datetime.now().strftime(\"saved_models/sparse_conv3d_model-%Y%m%d-%H%M%S.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1789d75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ani = plot_video(example_data[1][1])\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9d08cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=1\n",
    "activations = sparse_layer(example_data[1][idx:idx+1].to(device))\n",
    "reconstructions = sparse_layer.reconstructions(\n",
    "    activations).cpu().detach().numpy()\n",
    "\n",
    "ani = plot_video(reconstructions.squeeze(0))\n",
    "# ani = plot_original_vs_recon(example_data[1][idx:idx+1], reconstructions, idx=0)\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea8dc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ani = plot_filters(sparse_layer.filters.cpu().detach())\n",
    "# HTML(ani.to_html5_video())\n",
    "ani.save('saved_models/frame_vis.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac944fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SmallDataClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, sparse_layer):\n",
    "        super().__init__()\n",
    "\n",
    "        self.sparse_layer = sparse_layer\n",
    "        \n",
    "        self.pool = nn.MaxPool3d(2, 2)\n",
    "        \n",
    "        self.dropout3d = torch.nn.Dropout3d(p=0.1, inplace=False)\n",
    "        self.dropout = torch.nn.Dropout(p=0.5, inplace=False)\n",
    "        \n",
    "        # First fully connected layer\n",
    "        self.fc1 = nn.Linear(3272808, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    # x represents our data\n",
    "    def forward(self, x):\n",
    "        # Pass data through conv1\n",
    "        activations = self.sparse_layer(x)\n",
    "        \n",
    "        # x = self.dropout3d(x)\n",
    "        \n",
    "        # Flatten x with start_dim=1\n",
    "        x = torch.flatten(activations, 1)\n",
    "        \n",
    "        # print(x.shape)\n",
    "        \n",
    "        # Pass data through fc1\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8cd17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a frozen sparse layer then add a small data classifier on top\n",
    "frozen_sparse = ConvSparseLayer(in_channels=1,\n",
    "                               out_channels=24,\n",
    "                               kernel_size=16,\n",
    "                               stride=2,\n",
    "                               padding=0,\n",
    "                               convo_dim=3,\n",
    "                               rectifier=True,\n",
    "                               shrink=0.05,\n",
    "                               lam=0.05,\n",
    "                               max_activation_iter=300,\n",
    "                               activation_lr=1e-2)\n",
    "sparse_param = torch.load(\"saved_models/sparse_conv3d_model-best.pt\")\n",
    "frozen_sparse.load_state_dict(sparse_param['model_state_dict'])\n",
    "        \n",
    "for param in frozen_sparse.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "predictive_model = torch.nn.DataParallel(SmallDataClassifier(frozen_sparse), device_ids=[0,1,2,3])\n",
    "predictive_model.to(device)\n",
    "\n",
    "learning_rate = 1e-2\n",
    "prediction_optimizer = torch.optim.Adam(predictive_model.parameters(),\n",
    "                                        lr=learning_rate)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f5a6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "predictive_model.to(device)\n",
    "\n",
    "idx=3\n",
    "predictive_model(example_data[1][idx:idx+1].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897a512b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for epoch in range(15):\n",
    "    epoch_loss = 0\n",
    "    # for local_batch in train_loader:\n",
    "    t1 = time.perf_counter()\n",
    "    for labels, local_batch in train_loader:\n",
    "        local_batch = local_batch.to(device)\n",
    "        \n",
    "        torch_labels = torch.zeros(len(labels))\n",
    "        torch_labels[[i for i in range(len(labels)) if labels[i] == 'PTX_No_Sliding']] = 1\n",
    "        torch_labels = torch_labels.unsqueeze(1).to(device)\n",
    "        \n",
    "        pred, activations = predictive_model(local_batch)\n",
    "        \n",
    "        loss = criterion(pred, torch_labels)\n",
    "        # loss += frozen_sparse.loss(local_batch, activations)\n",
    "        epoch_loss += loss.item() * local_batch.size(0)\n",
    "        \n",
    "        prediction_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        prediction_optimizer.step()\n",
    "        \n",
    "    t2 = time.perf_counter()\n",
    "    print('epoch={}, time={:.2f}, loss={:.2f}'.format(epoch, t2-t1, epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2609584f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    y_h = None\n",
    "    y = None\n",
    "    \n",
    "    error = None\n",
    "    \n",
    "    t1 = time.perf_counter()\n",
    "    # for local_batch in train_loader:\n",
    "    for labels, local_batch in test_loader:\n",
    "        local_batch = local_batch.to(device)\n",
    "\n",
    "        torch_labels = torch.zeros(len(labels))\n",
    "        torch_labels[[i for i in range(len(labels)) if labels[i] == 'PTX_No_Sliding']] = 1\n",
    "        torch_labels = torch_labels.unsqueeze(1).to(device)\n",
    "\n",
    "        \n",
    "        pred, _ = predictive_model(local_batch)\n",
    "        \n",
    "        loss = criterion(pred, torch_labels)\n",
    "        epoch_loss += loss.item() * local_batch.size(0)\n",
    "\n",
    "        if error is None:\n",
    "            error = torch.abs(torch_labels - torch.nn.Sigmoid()(pred).round()).flatten()\n",
    "            y_h = torch.nn.Sigmoid()(pred).round().flatten()\n",
    "            y = torch_labels.flatten()\n",
    "        else:\n",
    "            error = torch.cat((error, torch.abs(torch_labels - torch.nn.Sigmoid()(pred).round()).flatten()))\n",
    "            y_h = torch.cat((y_h, torch.nn.Sigmoid()(pred).round().flatten()))\n",
    "            y = torch.cat((y, torch_labels.flatten()))\n",
    "            \n",
    "    t2 = time.perf_counter()\n",
    "    \n",
    "    print('loss={:.2f}, time={:.2f}'.format(loss, t2-t1))\n",
    "        \n",
    "    print(\"Overall error={:.2f}\".format(error.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8017a9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y.cpu(), y_h.cpu())\n",
    "\n",
    "cm_display = ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1dab96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9053bc7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4438f631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from conv_sparse_model import ConvSparseLayer\n",
    "from small_data_classifier import SmallDataClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from train_conv3d_sparse_model import load_balls_data\n",
    "from train_conv3d_sparse_model import plot_original_vs_recon\n",
    "from train_conv3d_sparse_model import plot_filters\n",
    "from train_conv3d_sparse_model import plot_video\n",
    "\n",
    "from load_data import load_bamc_data, load_covid_data\n",
    "\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921410f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == \"cpu\":\n",
    "    batch_size = 1\n",
    "else:\n",
    "    batch_size = 4*3\n",
    "    # batch_size = 3\n",
    "\n",
    "# train_loader = load_balls_data(batch_size)\n",
    "train_loader, test_loader = load_bamc_data(batch_size)\n",
    "print('Loaded', len(train_loader), 'train examples')\n",
    "print('Loaded', len(test_loader), 'test examples')\n",
    "\n",
    "example_data = next(iter(train_loader))\n",
    "\n",
    "sparse_layer = ConvSparseLayer(in_channels=1,\n",
    "                               out_channels=16,\n",
    "                               kernel_size=(4, 16, 16),\n",
    "                               stride=2,\n",
    "                               padding=0,\n",
    "                               convo_dim=3,\n",
    "                               rectifier=True,\n",
    "                               lam=0.015,\n",
    "                               max_activation_iter=1000,\n",
    "                               activation_lr=1e-4)\n",
    "model = sparse_layer\n",
    "model = torch.nn.DataParallel(model, device_ids=[0, 1, 2, 3])\n",
    "model.to(device)\n",
    "\n",
    "learning_rate = 3e-4\n",
    "optimizer = torch.optim.Adam(sparse_layer.parameters(),\n",
    "                                    lr=learning_rate)\n",
    "#optimizer = torch.optim.SGD(sparse_layer.parameters(),\n",
    "#                            momentum=0.9,\n",
    "#                            lr=learning_rate)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "# scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-14, max_lr=1e-5,step_size_up=20,mode=\"triangular2\")\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29f1e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models if we'd like to\n",
    "checkpoint = torch.load(\"lower_lam/sparse_conv3d_model-best.pt\")\n",
    "model.module.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# Put everything on the target device\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48505f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a23948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for g in optimizer.param_groups:\n",
    "#     g['lr'] = 1e-5\n",
    "# sparse_layer.lam = 0.1\n",
    "# sparse_layer.shrink = 0.1\n",
    "\n",
    "loss_log = []\n",
    "best_so_far = float('inf')\n",
    "\n",
    "for epoch in range(300):\n",
    "    epoch_loss = 0\n",
    "    epoch_start = time.perf_counter()\n",
    "    # for local_batch in train_loader:\n",
    "    for labels, local_batch in train_loader:\n",
    "        local_batch = local_batch.to(device)\n",
    "        \n",
    "        # pred, activations = model(local_batch)\n",
    "        activations = model(local_batch)\n",
    "        loss = sparse_layer.loss(local_batch, activations)\n",
    "        # loss += criterion(pred, torch_labels)\n",
    "        # print('epoch={}, loss={:.2f}, time={:.2f}'.format(epoch, loss, t2-t1))\n",
    "        epoch_loss += loss.item() * local_batch.size(0)\n",
    "        # print('l:', loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        sparse_layer.normalize_weights()\n",
    "    \n",
    "    epoch_end = time.perf_counter()    \n",
    "    epoch_loss /= len(train_loader.sampler)\n",
    "    \n",
    "    if epoch_loss < best_so_far:\n",
    "        print(\"found better model\")\n",
    "        # Save model parameters\n",
    "        torch.save({\n",
    "            'model_state_dict': model.module.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, datetime.now().strftime(\"saved_models/sparse_conv3d_model-best.pt\"))\n",
    "        best_so_far = epoch_loss\n",
    "        \n",
    "    loss_log.append(epoch_loss)\n",
    "    print('epoch={}, epoch_loss={:.2f}, time={:.2f}'.format(epoch, epoch_loss, epoch_end - epoch_start))\n",
    "    # scheduler.step(epoch_loss)\n",
    "    # scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d178b10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44991a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93a7a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model parameters\n",
    "torch.save({\n",
    "    'model_state_dict': model.module.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, datetime.now().strftime(\"saved_models/sparse_conv3d_model-%Y%m%d-%H%M%S.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edd82dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ani = plot_video(example_data[1][1])\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9efce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=1\n",
    "u_init = torch.zeros([1, sparse_layer.out_channels] +\n",
    "                    sparse_layer.get_output_shape(example_data[1]))\n",
    "\n",
    "activations, _ = sparse_layer(example_data[1][idx:idx+1].to(device), u_init)\n",
    "print(torch.count_nonzero(activations) / torch.numel(activations))\n",
    "reconstructions = sparse_layer.reconstructions(\n",
    "    activations).cpu().detach().numpy()\n",
    "\n",
    "ani = plot_video(reconstructions.squeeze(0))\n",
    "# ani = plot_original_vs_recon(example_data[1][idx:idx+1], reconstructions, idx=0)\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3eecef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ani = plot_filters(sparse_layer.filters.cpu().detach())\n",
    "HTML(ani.to_html5_video())\n",
    "# ani.save('lower_lam/frame_vis.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945724ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SmallDataClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, sparse_layer):\n",
    "        super().__init__()\n",
    "\n",
    "        self.sparse_layer = sparse_layer\n",
    "        \n",
    "        self.pool = nn.MaxPool3d(2, 2)\n",
    "        \n",
    "        self.dropout3d = torch.nn.Dropout3d(p=0.1, inplace=False)\n",
    "        self.dropout = torch.nn.Dropout(p=0.5, inplace=False)\n",
    "        \n",
    "        # First fully connected layer\n",
    "        self.fc1 = nn.Linear(3272808, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    # x represents our data\n",
    "    def forward(self, x):\n",
    "        # Pass data through conv1\n",
    "        activations = self.sparse_layer(x)\n",
    "        \n",
    "        # x = self.dropout3d(x)\n",
    "        \n",
    "        # Flatten x with start_dim=1\n",
    "        x = torch.flatten(activations, 1)\n",
    "        \n",
    "        # print(x.shape)\n",
    "        \n",
    "        # Pass data through fc1\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036221ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a frozen sparse layer then add a small data classifier on top\n",
    "frozen_sparse = ConvSparseLayer(in_channels=1,\n",
    "                               out_channels=24,\n",
    "                               kernel_size=16,\n",
    "                               stride=2,\n",
    "                               padding=0,\n",
    "                               convo_dim=3,\n",
    "                               rectifier=True,\n",
    "                               shrink=0.05,\n",
    "                               lam=0.05,\n",
    "                               max_activation_iter=300,\n",
    "                               activation_lr=1e-2)\n",
    "sparse_param = torch.load(\"saved_models/sparse_conv3d_model-best.pt\")\n",
    "frozen_sparse.load_state_dict(sparse_param['model_state_dict'])\n",
    "        \n",
    "for param in frozen_sparse.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "predictive_model = torch.nn.DataParallel(SmallDataClassifier(frozen_sparse), device_ids=[0,1,2,3])\n",
    "predictive_model.to(device)\n",
    "\n",
    "learning_rate = 1e-2\n",
    "prediction_optimizer = torch.optim.Adam(predictive_model.parameters(),\n",
    "                                        lr=learning_rate)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bb9279",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "predictive_model.to(device)\n",
    "\n",
    "idx=3\n",
    "predictive_model(example_data[1][idx:idx+1].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edfcb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for epoch in range(15):\n",
    "    epoch_loss = 0\n",
    "    # for local_batch in train_loader:\n",
    "    t1 = time.perf_counter()\n",
    "    for labels, local_batch in train_loader:\n",
    "        local_batch = local_batch.to(device)\n",
    "        \n",
    "        torch_labels = torch.zeros(len(labels))\n",
    "        torch_labels[[i for i in range(len(labels)) if labels[i] == 'PTX_No_Sliding']] = 1\n",
    "        torch_labels = torch_labels.unsqueeze(1).to(device)\n",
    "        \n",
    "        pred, activations = predictive_model(local_batch)\n",
    "        \n",
    "        loss = criterion(pred, torch_labels)\n",
    "        # loss += frozen_sparse.loss(local_batch, activations)\n",
    "        epoch_loss += loss.item() * local_batch.size(0)\n",
    "        \n",
    "        prediction_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        prediction_optimizer.step()\n",
    "        \n",
    "    t2 = time.perf_counter()\n",
    "    print('epoch={}, time={:.2f}, loss={:.2f}'.format(epoch, t2-t1, epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7164e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    y_h = None\n",
    "    y = None\n",
    "    \n",
    "    error = None\n",
    "    \n",
    "    t1 = time.perf_counter()\n",
    "    # for local_batch in train_loader:\n",
    "    for labels, local_batch in test_loader:\n",
    "        local_batch = local_batch.to(device)\n",
    "\n",
    "        torch_labels = torch.zeros(len(labels))\n",
    "        torch_labels[[i for i in range(len(labels)) if labels[i] == 'PTX_No_Sliding']] = 1\n",
    "        torch_labels = torch_labels.unsqueeze(1).to(device)\n",
    "\n",
    "        \n",
    "        pred, _ = predictive_model(local_batch)\n",
    "        \n",
    "        loss = criterion(pred, torch_labels)\n",
    "        epoch_loss += loss.item() * local_batch.size(0)\n",
    "\n",
    "        if error is None:\n",
    "            error = torch.abs(torch_labels - torch.nn.Sigmoid()(pred).round()).flatten()\n",
    "            y_h = torch.nn.Sigmoid()(pred).round().flatten()\n",
    "            y = torch_labels.flatten()\n",
    "        else:\n",
    "            error = torch.cat((error, torch.abs(torch_labels - torch.nn.Sigmoid()(pred).round()).flatten()))\n",
    "            y_h = torch.cat((y_h, torch.nn.Sigmoid()(pred).round().flatten()))\n",
    "            y = torch.cat((y, torch_labels.flatten()))\n",
    "            \n",
    "    t2 = time.perf_counter()\n",
    "    \n",
    "    print('loss={:.2f}, time={:.2f}'.format(loss, t2-t1))\n",
    "        \n",
    "    print(\"Overall error={:.2f}\".format(error.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bbd702",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y.cpu(), y_h.cpu())\n",
    "\n",
    "cm_display = ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ee7479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87504ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d5a2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from conv_sparse_model import ConvSparseLayer\n",
    "from small_data_classifier import SmallDataClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from train_conv3d_sparse_model import load_balls_data\n",
    "from train_conv3d_sparse_model import plot_original_vs_recon\n",
    "from train_conv3d_sparse_model import plot_filters\n",
    "from train_conv3d_sparse_model import plot_video\n",
    "\n",
    "from load_data import load_bamc_data, load_covid_data\n",
    "\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c94abf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == \"cpu\":\n",
    "    batch_size = 1\n",
    "else:\n",
    "    batch_size = 4*3\n",
    "    # batch_size = 3\n",
    "\n",
    "# train_loader = load_balls_data(batch_size)\n",
    "train_loader, test_loader = load_bamc_data(batch_size)\n",
    "print('Loaded', len(train_loader), 'train examples')\n",
    "print('Loaded', len(test_loader), 'test examples')\n",
    "\n",
    "example_data = next(iter(train_loader))\n",
    "\n",
    "sparse_layer = ConvSparseLayer(in_channels=1,\n",
    "                               out_channels=24,\n",
    "                               kernel_size=16,\n",
    "                               stride=2,\n",
    "                               padding=0,\n",
    "                               convo_dim=3,\n",
    "                               rectifier=True,\n",
    "                               shrink=0.05,\n",
    "                               lam=0.05,\n",
    "                               max_activation_iter=300,\n",
    "                               activation_lr=1e-2)\n",
    "model = sparse_layer\n",
    "model = torch.nn.DataParallel(model, device_ids=[0, 1, 2, 3])\n",
    "model.to(device)\n",
    "\n",
    "learning_rate = 3e-4\n",
    "optimizer = torch.optim.Adam(sparse_layer.parameters(),\n",
    "                                    lr=learning_rate)\n",
    "#optimizer = torch.optim.SGD(sparse_layer.parameters(),\n",
    "#                            momentum=0.9,\n",
    "#                            lr=learning_rate)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "# scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-14, max_lr=1e-5,step_size_up=20,mode=\"triangular2\")\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674f161a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models if we'd like to\n",
    "checkpoint = torch.load(\"saved_models/sparse_conv3d_model-best.pt\")\n",
    "model.module.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# Put everything on the target device\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9a9058",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8c0577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for g in optimizer.param_groups:\n",
    "#     g['lr'] = 1e-5\n",
    "# sparse_layer.lam = 0.1\n",
    "# sparse_layer.shrink = 0.1\n",
    "\n",
    "loss_log = []\n",
    "best_so_far = float('inf')\n",
    "\n",
    "for epoch in range(300):\n",
    "    epoch_loss = 0\n",
    "    epoch_start = time.perf_counter()\n",
    "    # for local_batch in train_loader:\n",
    "    for labels, local_batch in train_loader:\n",
    "        local_batch = local_batch.to(device)\n",
    "        \n",
    "        # pred, activations = model(local_batch)\n",
    "        activations = model(local_batch)\n",
    "        loss = sparse_layer.loss(local_batch, activations)\n",
    "        # loss += criterion(pred, torch_labels)\n",
    "        # print('epoch={}, loss={:.2f}, time={:.2f}'.format(epoch, loss, t2-t1))\n",
    "        epoch_loss += loss.item() * local_batch.size(0)\n",
    "        # print('l:', loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        sparse_layer.normalize_weights()\n",
    "    \n",
    "    epoch_end = time.perf_counter()    \n",
    "    epoch_loss /= len(train_loader.sampler)\n",
    "    \n",
    "    if epoch_loss < best_so_far:\n",
    "        print(\"found better model\")\n",
    "        # Save model parameters\n",
    "        torch.save({\n",
    "            'model_state_dict': model.module.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, datetime.now().strftime(\"saved_models/sparse_conv3d_model-best.pt\"))\n",
    "        best_so_far = epoch_loss\n",
    "        \n",
    "    loss_log.append(epoch_loss)\n",
    "    print('epoch={}, epoch_loss={:.2f}, time={:.2f}'.format(epoch, epoch_loss, epoch_end - epoch_start))\n",
    "    # scheduler.step(epoch_loss)\n",
    "    # scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e69ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e4110c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb3f096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model parameters\n",
    "torch.save({\n",
    "    'model_state_dict': model.module.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, datetime.now().strftime(\"saved_models/sparse_conv3d_model-%Y%m%d-%H%M%S.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a293fe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "ani = plot_video(example_data[1][1])\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8514babf",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=1\n",
    "activations = sparse_layer(example_data[1][idx:idx+1].to(device))\n",
    "reconstructions = sparse_layer.reconstructions(\n",
    "    activations).cpu().detach().numpy()\n",
    "\n",
    "ani = plot_video(reconstructions.squeeze(0))\n",
    "# ani = plot_original_vs_recon(example_data[1][idx:idx+1], reconstructions, idx=0)\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8a4b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "ani = plot_filters(sparse_layer.filters.cpu().detach())\n",
    "# HTML(ani.to_html5_video())\n",
    "ani.save('saved_models/frame_vis.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4f77a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SmallDataClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, sparse_layer):\n",
    "        super().__init__()\n",
    "\n",
    "        self.sparse_layer = sparse_layer\n",
    "        \n",
    "        self.pool = nn.MaxPool3d(2, 2)\n",
    "        \n",
    "        self.dropout3d = torch.nn.Dropout3d(p=0.1, inplace=False)\n",
    "        self.dropout = torch.nn.Dropout(p=0.5, inplace=False)\n",
    "        \n",
    "        # First fully connected layer\n",
    "        self.fc1 = nn.Linear(3272808, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    # x represents our data\n",
    "    def forward(self, x):\n",
    "        # Pass data through conv1\n",
    "        activations = self.sparse_layer(x)\n",
    "        \n",
    "        # x = self.dropout3d(x)\n",
    "        \n",
    "        # Flatten x with start_dim=1\n",
    "        x = torch.flatten(activations, 1)\n",
    "        \n",
    "        # print(x.shape)\n",
    "        \n",
    "        # Pass data through fc1\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec1d913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a frozen sparse layer then add a small data classifier on top\n",
    "frozen_sparse = ConvSparseLayer(in_channels=1,\n",
    "                               out_channels=24,\n",
    "                               kernel_size=16,\n",
    "                               stride=2,\n",
    "                               padding=0,\n",
    "                               convo_dim=3,\n",
    "                               rectifier=True,\n",
    "                               shrink=0.05,\n",
    "                               lam=0.05,\n",
    "                               max_activation_iter=300,\n",
    "                               activation_lr=1e-2)\n",
    "sparse_param = torch.load(\"saved_models/sparse_conv3d_model-best.pt\")\n",
    "frozen_sparse.load_state_dict(sparse_param['model_state_dict'])\n",
    "        \n",
    "for param in frozen_sparse.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "predictive_model = torch.nn.DataParallel(SmallDataClassifier(frozen_sparse), device_ids=[0,1,2,3])\n",
    "predictive_model.to(device)\n",
    "\n",
    "learning_rate = 1e-2\n",
    "prediction_optimizer = torch.optim.Adam(predictive_model.parameters(),\n",
    "                                        lr=learning_rate)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8cbeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "predictive_model.to(device)\n",
    "\n",
    "idx=3\n",
    "predictive_model(example_data[1][idx:idx+1].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2af51d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for epoch in range(15):\n",
    "    epoch_loss = 0\n",
    "    # for local_batch in train_loader:\n",
    "    t1 = time.perf_counter()\n",
    "    for labels, local_batch in train_loader:\n",
    "        local_batch = local_batch.to(device)\n",
    "        \n",
    "        torch_labels = torch.zeros(len(labels))\n",
    "        torch_labels[[i for i in range(len(labels)) if labels[i] == 'PTX_No_Sliding']] = 1\n",
    "        torch_labels = torch_labels.unsqueeze(1).to(device)\n",
    "        \n",
    "        pred, activations = predictive_model(local_batch)\n",
    "        \n",
    "        loss = criterion(pred, torch_labels)\n",
    "        # loss += frozen_sparse.loss(local_batch, activations)\n",
    "        epoch_loss += loss.item() * local_batch.size(0)\n",
    "        \n",
    "        prediction_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        prediction_optimizer.step()\n",
    "        \n",
    "    t2 = time.perf_counter()\n",
    "    print('epoch={}, time={:.2f}, loss={:.2f}'.format(epoch, t2-t1, epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f440f59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    y_h = None\n",
    "    y = None\n",
    "    \n",
    "    error = None\n",
    "    \n",
    "    t1 = time.perf_counter()\n",
    "    # for local_batch in train_loader:\n",
    "    for labels, local_batch in test_loader:\n",
    "        local_batch = local_batch.to(device)\n",
    "\n",
    "        torch_labels = torch.zeros(len(labels))\n",
    "        torch_labels[[i for i in range(len(labels)) if labels[i] == 'PTX_No_Sliding']] = 1\n",
    "        torch_labels = torch_labels.unsqueeze(1).to(device)\n",
    "\n",
    "        \n",
    "        pred, _ = predictive_model(local_batch)\n",
    "        \n",
    "        loss = criterion(pred, torch_labels)\n",
    "        epoch_loss += loss.item() * local_batch.size(0)\n",
    "\n",
    "        if error is None:\n",
    "            error = torch.abs(torch_labels - torch.nn.Sigmoid()(pred).round()).flatten()\n",
    "            y_h = torch.nn.Sigmoid()(pred).round().flatten()\n",
    "            y = torch_labels.flatten()\n",
    "        else:\n",
    "            error = torch.cat((error, torch.abs(torch_labels - torch.nn.Sigmoid()(pred).round()).flatten()))\n",
    "            y_h = torch.cat((y_h, torch.nn.Sigmoid()(pred).round().flatten()))\n",
    "            y = torch.cat((y, torch_labels.flatten()))\n",
    "            \n",
    "    t2 = time.perf_counter()\n",
    "    \n",
    "    print('loss={:.2f}, time={:.2f}'.format(loss, t2-t1))\n",
    "        \n",
    "    print(\"Overall error={:.2f}\".format(error.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09ed95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y.cpu(), y_h.cpu())\n",
    "\n",
    "cm_display = ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c700680f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5dd93d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pocus_project)",
   "language": "python",
   "name": "darryl_pocus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
