{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0285a1bd-83ef-44b7-9654-012294fd5653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/home/dwh48@drexel.edu/sparse_coding_torch')\n",
    "\n",
    "from feature_extraction.conv_sparse_model import ConvSparseLayer\n",
    "from data_classifiers.small_data_classifier import SmallDataClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from feature_extraction.train_conv3d_sparse_model import load_balls_data\n",
    "from feature_extraction.train_conv3d_sparse_model import plot_original_vs_recon\n",
    "from feature_extraction.train_conv3d_sparse_model import plot_filters\n",
    "from feature_extraction.train_conv3d_sparse_model import plot_video\n",
    "\n",
    "from utils.load_data import load_bamc_clips, load_covid_data\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aba945d-1794-4d47-b6b5-3577803347d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 1\n",
    "    # batch_size = 3\n",
    "\n",
    "# train_loader = load_balls_data(batch_size)\n",
    "train_loader, _ = load_bamc_clips(batch_size, 1.0, sparse_model=None, device=None, num_frames=4, seed=42)\n",
    "print('Loaded', len(train_loader), 'train examples')\n",
    "\n",
    "example_data = next(iter(train_loader))\n",
    "\n",
    "sparse_layer = ConvSparseLayer(in_channels=1,\n",
    "                               out_channels=64,\n",
    "                               kernel_size=(4, 16, 16),\n",
    "                               stride=1,\n",
    "                               padding=0,\n",
    "                               convo_dim=3,\n",
    "                               rectifier=True,\n",
    "                               lam=0.05,\n",
    "                               max_activation_iter=75,\n",
    "                               activation_lr=1e-2)\n",
    "sparse_layer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac3f6af-1f40-47ed-bf65-ccb0253b3066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models if we'd like to\n",
    "checkpoint = torch.load(\"/home/dwh48@drexel.edu/sparse_coding_torch/model-20211027-034737.pt\")\n",
    "sparse_layer.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ce2015-183f-4f88-98b2-7cc23790c1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_ids = ['/shared_data/bamc_data_scale_cropped/PTX_Sliding/image_104548309385533_CLEAN.mov', '/shared_data/bamc_data_scale_cropped/PTX_Sliding/image_588413346180_CLEAN.mp4', '/shared_data/bamc_data_scale_cropped/PTX_Sliding/image_24164968068436_CLEAN.mp4', '/shared_data/bamc_data_scale_cropped/PTX_Sliding/image_104543812690743_CLEAN.mov', '/shared_data/bamc_data_scale_cropped/PTX_Sliding/image_1499268364374_clean.mov', '/shared_data/bamc_data_scale_cropped/PTX_Sliding/image_1511338287338_clean.mov']\n",
    "fn_ids = ['/shared_data/bamc_data_scale_cropped/PTX_No_Sliding/image_610066411380_CLEAN.mov', '/shared_data/bamc_data_scale_cropped/PTX_No_Sliding/image_642169070951_clean.mp4', '/shared_data/bamc_data_scale_cropped/PTX_No_Sliding/image_1543571117118_clean.mp4', '/shared_data/bamc_data_scale_cropped/PTX_No_Sliding/image_6056976176281_CLEAN.mov', '/shared_data/bamc_data_scale_cropped/PTX_No_Sliding/image_27185428518326_CLEAN.mp4', '/shared_data/bamc_data_scale_cropped/PTX_No_Sliding/image_588695055398_clean.mov', '/shared_data/bamc_data_scale_cropped/PTX_No_Sliding/image_2418161753608_clean.mp4', '/shared_data/bamc_data_scale_cropped/PTX_No_Sliding/image_2454526567135_CLEAN.mp4', '/shared_data/bamc_data_scale_cropped/PTX_No_Sliding/image_584357289931_clean.mov', '/shared_data/bamc_data_scale_cropped/PTX_No_Sliding/image_27180764486244_CLEAN.mp4', '/shared_data/bamc_data_scale_cropped/PTX_No_Sliding/image_1884162273498_clean.mov', '/shared_data/bamc_data_scale_cropped/PTX_No_Sliding/image_417221672548_CLEAN.mp4', '/shared_data/bamc_data_scale_cropped/PTX_No_Sliding/image_426794579576_CLEAN.mp4', '/shared_data/bamc_data_scale_cropped/PTX_No_Sliding/image_1895283541879_clean.mov']\n",
    "\n",
    "incorrect_sparsity = []\n",
    "correct_sparsity = []\n",
    "incorrect_filter_act = torch.zeros(64)\n",
    "correct_filter_act = torch.zeros(64)\n",
    "\n",
    "for labels, local_batch, vid_f in tqdm(train_loader):\n",
    "    u_init = torch.zeros([1, sparse_layer.out_channels] +\n",
    "                    sparse_layer.get_output_shape(example_data[1])).to(device)\n",
    "\n",
    "    activations, _ = sparse_layer(local_batch.to(device), u_init)\n",
    "    sparsity = torch.count_nonzero(activations) / torch.numel(activations)\n",
    "    filter_act = torch.sum(activations.squeeze(), dim=[1, 2])\n",
    "    filter_act = filter_act / torch.max(filter_act)\n",
    "    filter_act = filter_act.detach().cpu()\n",
    "    \n",
    "    if vid_f[0] in fp_ids or vid_f[0] in fn_ids:\n",
    "        incorrect_sparsity.append(sparsity)\n",
    "        incorrect_filter_act += filter_act\n",
    "    else:\n",
    "        correct_sparsity.append(sparsity)\n",
    "        correct_filter_act += filter_act\n",
    "        \n",
    "print(torch.mean(torch.tensor(correct_sparsity)))\n",
    "print(torch.mean(torch.tensor(incorrect_sparsity)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b8dfc9-9736-4b1e-bb4b-de6b69056103",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b202266-6b44-4c8d-9442-b86e6ad9b11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = sparse_layer.filters.cpu().detach()\n",
    "print(filters.size())\n",
    "\n",
    "filters = torch.stack([filters[val] for val in correct_filter_act.argsort(descending=True)])\n",
    "\n",
    "print(filters.size())\n",
    "\n",
    "ani = plot_filters(filters)\n",
    "# HTML(ani.to_html5_video())\n",
    "ani.save(\"/home/dwh48@drexel.edu/sparse_coding_torch/data_classifiers/outputs/kfold_3dcnn/correct_vis.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d1f47c-ba4c-4c3a-9f96-4901c39e16e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pocus_project)",
   "language": "python",
   "name": "darryl_pocus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
